---
title: "Les biais de genres systémiques dans les IA génératives."
description: "Une récente étude de l'University College London (UCL) révèle que les outils d'intelligence artificielle (IA) les plus populaires font preuve de discrimination à l'égard des femmes et des personnes issues de différentes cultures et sexualités."
date: 21-04-2024
lang: fr
categories: 
  - stats
  - IA generatives
  - sociologie
  - genre
draft: false
number-sections: false
image: emnist-manual-loading-thumbnail.png
bibliography: references.bib
format:
  html:
    fig-cap-location: bottom
    include-before-body: ../../html/margin_image.html
    include-after-body: ../../html/blog_footer.html
    comments: false
editor: 
  markdown: 
    wrap: sentence
---

```{r}
#| include: false
personnages <- read.csv2("personnages.csv")
library('dplyr')
library('questionr')
library('ggplot2')
library('ggwordcloud')
```

## Un biais systémique relevé par une étude.

Une récente étude [voir @etudeUCL] menée par des chercheurs de l'University College London (UCL) révèle que les outils d'intelligence artificielle (IA) les plus populaires font preuve de discrimination à l'égard des femmes et des personnes issues de différentes cultures et sexualités.\
L'étude, commandée et publiée par l'UNESCO, a examiné les stéréotypes présents dans les LLM (Large Language Models), outils de traitement du langage naturel sur lesquels se basent les plateformes génératives d'IA les plus populaires, notamment Open AI's GPT-3.5 et GPT-2 et META's Llama 2.\
Les résultats dévoilent que les noms féminins sont associés à des rôles de genre traditionnels, tels que "famille", "enfants" et "mari", tandis que les noms masculins sont associés à des mots tels que "carrière", "dirigeants", "gestion" et "entreprise".
Les textes générés par l'IA ont également montré des stéréotypes négatifs basés sur la culture ou la sexualité.\
Par exemple, les histoires générées par Llama 2 placent les femmes dans des rôles domestiques quatre fois plus souvent que les hommes.
Ces derniers se voient attribués des emplois plus divers et de haut statut.\
Une des raisons avancée tient à la prédominance de ces biais dans les ressources utilisées pour construire ces modèles.
La professeure Ivana Drobnjak, auteure du rapport de l'UCL Computer Science et membre de l'équipe de la Chaire UNESCO en IA à l'UCL, avance cette explication: "L'IA apprend d'Internet et des données historiques et prend des décisions en fonction de ces connaissances, qui sont souvent biaisées. Le fait que les femmes n'aient pas été aussi présentes que les hommes dans les sciences et l'ingénierie dans le passé, par exemple, ne signifie pas qu'elles sont de moins bonnes scientifiques et ingénieures. Nous devons guider ces algorithmes pour qu'ils apprennent l'égalité, l'équité et les droits de l'homme, afin qu'ils prennent de meilleures décisions."\
Les biais de genre sont si profondément ancrés dans les LLM qu'une refonte éthique dans le développement de l'IA est absolument nécessaire.
L'équipe de la Chaire UNESCO en IA à l'UCL travaillera avec l'UNESCO pour sensibiliser à ce problème et contribuer au développement de solutions en organisant des ateliers et des évènements conjoints impliquant des parties prenantes pertinentes.\
Le rapport a été présenté lors de la réunion de dialogue sur la transformation numérique de l'UNESCO le 6 mars 2024 et lors de la 68e session de la Commission de la condition de la femme des Nations Unies.
En conclusion, les chercheurs recommandent que l'IA doit être guidée pour apprendre l'égalité, l'équité et les droits de l'homme afin de prendre de meilleures décisions.

## Génération de textes

J'ai voulu effectuer une rapide vérification en utilisant l'IA générative française Mistral (<https://chat.mistral.ai/>).
Je lui ai demandé de générer une liste de personnages fictifs en établissant leur biographie.
Les informations recueillies sont dans le fichier `personnages.csv`, obtenu après retraitement.

Par exemple:

```{r}
#| echo: false
#| warning: false

personnages <- read.csv2("personnages.csv")
library('dplyr')
personnages %>% 
  slice(1) %>% 
  knitr::kable()
```

## Analyse

### La profession est-elle indépendante du sexe du personnage ?

On va croiser la profession (variable `Occupation`) avec `Sexe` (codée H pour hommes, et F pour femmes).
On obtient le tableau @tbl-OccupSexe

```{r}
#| label: tbl-OccupSexe
#| tbl-cap: "OccupSexe"
#| cap-location: top
#| warning: false

tab_OccupSexe <- table(personnages$Occupation, personnages$Sexe)

library('knitr') 
kable(tab_OccupSexe)
```

On va utiliser le package *questionr* (voir @questionR) pour calculer les structures par sexe, ainsi que les marges du tableau.

```{r}
#| label: tbl_OccupSexe2
#| tbl-cap: "Lien entre le sexe et l'occupuation du personnage principal"
#| cap-location: top
#| warning: false

#install.packages("questionr")
library('questionr')
knitr::kable(round(cprop(tab_OccupSexe), 1))
```

Le tableau dévoile de nombreuses sur-représentations de genre et peu de parité.
Par exemple, les il y a 7,3% d'entrepreneurs parmi les personnages mais seulement 1,6% des personnages féminins exercent cette profession.
Calculons le V de Cramer (un avertissement indique que de nombreuses cases présentent peu d'effectifs).

Le V de Cramer est une mesure d'association utilisée pour évaluer la force de la relation entre deux variables qualitatives (catégorielles) dans un tableau de contingence.
Il est particulièrement utile pour des tableaux plus grands que 2×2, car il permet de comparer des relations entre variables ayant des nombres différents de catégories.

::: {callout-tip}
## Formule du V de Cramer

Le V de Cramer est calculé à partir de la statistique du $\chi^2$ (chi-carré) selon la formule suivante :
$V = \sqrt{\frac{\chi^2}{n \times min(k-1,r-1)}}$ où :

- $\chi^2$ est la statistique du test chi-carré,
- n est la taille de l'échantillon,
- k est le nombre de colonnes (modalités de la première variable),
- r est le nombre de lignes (modalités de la seconde variable),
- $min(k−1,r−1)$ est le minimum entre le nombre de colonnes moins un et le nombre de lignes moins un.

:::

### Interprétation de la Valeur du V de Cramer

La valeur du V de Cramer varie de 0 à 1 :

-   0 indique une absence totale d'association entre les deux variables.
-   1 indique une association parfaite entre les deux variables.

En général, les seuils d'interprétation ne sont pas stricts, mais on utilise souvent les conventions suivantes :

-   V < 0,2 : association faible,
-   0,2 ≤ V < 0,4 : association modérée,
-   0,4 ≤ V < 0,6 : association forte,
-   V ≥ 0,6 : association très forte.

Une valeur de `r cramer.v(tab_OccupSexe)` dans le cadre du croisement entre la profession et le sexe d'une population indique une association très forte entre ces deux variables.
Cela signifie qu'il existe une relation importante entre le sexe et la profession dans cette population : certaines professions sont probablement majoritairement exercées par un sexe plutôt que par l'autre.
Cependant, cette mesure ne donne pas d'indication sur le sens de la relation, elle indique seulement qu'une relation existe avec une intensité élevée.

### Des personnages féminins plutôt jeunes

Les personnages de cette population sont majoritairement âgés entre 30 et 40 ans:

```{r}
#| label: tab-trAge
#| echo: false
tab_trAge <- table(personnages$trAge)
round(tab_trAge/nrow(personnages)*100, 1) %>% knitr::kable(col.names = c("Âge", "%") )

```

```{r}
#| label: tab_trAge_Sexe

tab_trAge_Sexe <- table(personnages$Sexe, personnages$trAge)
cprop(tab_trAge_Sexe)
cramer.v(tab_trAge_Sexe)
```

Malgré un V de Cramer peu élevé, on relève des surreprésentations des jeunes parmi les femmes.
En effet, bienque la population des personnages compte autant d'hommes que de femmes, celles-ci représentent environ deux-tiers des moins de 30 ans.
C'est encore plus marqué dans la tranche d'âge des plus de 40 ans, où les femmes ne représentent que 12% de cette classe d'âge.

### Des thèmes très genrés

```{r}
#| label: tbl-theme
#| fig-cap: "Réprésentations de genre dans les thèmes abordés"

tab_GenreRoman <- table(personnages$Sexe,personnages$GenreRoman)
knitr::kable(round(lprop(tab_GenreRoman),1))
```

Lorsque l'on analyse les thèmes des œuvres selon le sexe, la prédominance de genre est présente. Le tableau @tbl-theme le démontre. Le V de Cramer vaut `r questionr::cramer.v(tab_GenreRoman)` ce qui est le signe d'un lien avéré et plutôt fort entre le sexe du personnage principal et le thème de l’œuvre.

### Les traits de personnalité selon le sexe du personnage

On représente les termes employés pour décrire la personnalité de personnage par des nuages de mots.

```{r}
#| echo: false
#| warning: false
#| layout-ncol: 2
#| fig-cap: 
#|   - "Termes caractérisant la personnalité des personnages masculins"
#|   - "Termes caractérisant la personnalité des personnages féminins"

personnages$Personnalites <- strsplit(personnages$Personnalite, ", ")

persoH <- personnages %>% 
  select(Sexe, Personnalites) %>% 
  filter(Sexe == "H") 
persoF <- personnages %>% 
  select(Sexe, Personnalites) %>% 
  filter(Sexe == "F") 

'%notin%' <- Negate('%in%')

mots <- bind_rows(
  data.frame(Sexe = rep("H", length(unlist(strsplit(unlist(persoH$Personnalites) , " ")))), mots = unlist(strsplit(unlist(persoH$Personnalites) , " "))),
  data.frame(Sexe = rep("F", length(unlist(strsplit(unlist(persoF$Personnalites) , " ")))), mots = unlist(strsplit(unlist(persoF$Personnalites) , " ")))
)

list_words <- c("à", "a", "du", "mal", "et", "le", "la", "les", "autres", "avec" ,"son" ,"dans", "une", "des", "un", "aux",
                "vie", "privée", "faire")


mots <- mots %>% filter(mots %notin% list_words)
foo <- mots %>% 
  group_by(Sexe, mots) %>% 
  summarise(nb = n()) %>% 
  arrange(desc(nb)) %>% 
  mutate(angle = 45 * sample(-2:2, n(), replace = TRUE, prob = c(1, 1, 4, 1, 1)))

library('ggplot2')
library('ggwordcloud')

foo %>% filter(Sexe == "H") %>% 
  ggplot(aes(label = mots, size = nb, color = nb, angle = angle)) +
  geom_text_wordcloud_area(
    mask = png::readPNG("homme.png")
  ) +
  scale_size_area(max_size = 20) +
  scale_color_distiller(palette = "PuBu") +
  theme_minimal() 

foo %>% filter(Sexe == "F") %>% 
  ggplot(aes(label = mots, size = nb, color = nb, angle = angle)) +
  geom_text_wordcloud_area(
    mask = png::readPNG("femme.png")
  ) +
  scale_size_area(max_size = 18) +
  scale_color_distiller(palette = "PuRd") +
  theme_minimal() 
```


## Conclusion

## Liens

### Données

Réalisées par [![](https://chat.mistral.ai/_next/image?url=%2F_next%2Fstatic%2Fmedia%2Fle-chat-logo-large-light.b98da676.png&w=384&q=75){width="170"}](https://chat.mistral.ai/chat) <https://raw.githubusercontent.com/RemiDumas/RStats/refs/heads/main/posts/biais-de-genre-IA/personnages.csv>

### Source

<https://techxplore.com/news/2024-04-large-language-generate-biased-content.html>

### References

::: {#refs}
:::
